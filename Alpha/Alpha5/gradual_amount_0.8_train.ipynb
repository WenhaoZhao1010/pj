{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37429a96-2c34-4da8-afe1-c3e3697340d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 渐进式剪枝.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.utils.prune as prune\n",
    "from vgg_quant import*\n",
    "     \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\") \n",
    "    \n",
    "batch_size = 128\n",
    "model = VGG16_quant()\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "print_freq = 100\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "\n",
    "TARGET_SPARSITY = 0.8  \n",
    "N_EPOCHS = 10\n",
    "PRUNE_STEPS = 4  \n",
    "\n",
    "if PRUNE_STEPS == 2:\n",
    "    PRUNE_SCHEDULE = {\n",
    "        5: 0.4,   \n",
    "        10: 0.8, \n",
    "    }\n",
    "    PATH_prune = \"gradual_2step_0.8_ep10_vgg_pruned.pth\"\n",
    "elif PRUNE_STEPS == 4:\n",
    "    PRUNE_SCHEDULE = {\n",
    "        2: 0.2,  \n",
    "        4: 0.4, \n",
    "        7: 0.6, \n",
    "        10: 0.8, \n",
    "    }\n",
    "    PATH_prune = \"gradual_4step_0.8_ep10_vgg_pruned.pth\"\n",
    "def apply_pruning_with_target_sparsity(model, target_sparsity):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantConv2d):\n",
    "            if hasattr(module, 'weight_orig'):\n",
    "                prune.remove(module, 'weight')\n",
    "            prune.l1_unstructured(module, name='weight', amount=target_sparsity)\n",
    "    \n",
    "    total_zeros = 0\n",
    "    total_params = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, QuantConv2d):\n",
    "            mask = module.weight_mask if hasattr(module, 'weight_mask') else (module.weight != 0)\n",
    "            total_zeros += (mask == 0).sum().item()\n",
    "            total_params += mask.numel()\n",
    "    \n",
    "    actual_sparsity = total_zeros / total_params if total_params > 0 else 0\n",
    "\n",
    "lr = 1e-1\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "load_best_valid_parameters = '4bit_VGG_best_valid_acc.pth'\n",
    "checkpoint = torch.load(load_best_valid_parameters)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.cuda()\n",
    "model.train()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "    train_loss = train_loss / len(trainloader.dataset)\n",
    "    print(f'Epoch: {epoch+1} \\tTraining Loss: {train_loss:.6f}')\n",
    "    \n",
    "    current_epoch = epoch + 1\n",
    "    if current_epoch in PRUNE_SCHEDULE:\n",
    "        target_sparsity = PRUNE_SCHEDULE[current_epoch]\n",
    "        apply_pruning_with_target_sparsity(model, target_sparsity)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': train_loss,\n",
    "        'prune_schedule': PRUNE_SCHEDULE,\n",
    "        'target_sparsity': TARGET_SPARSITY,\n",
    "    }, PATH_prune)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"渐进式剪枝训练完成! 最终稀疏度: {TARGET_SPARSITY*100:.0f}%\")\n",
    "print(f\"模型已保存到: {PATH_prune}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91060e3a-0747-486b-b283-c55f9fecc2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
